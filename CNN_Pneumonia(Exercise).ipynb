{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "CNN_Pneumonia(Exercise).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AIWintermuteAI/Bertelsmann-Tech-Scholarship-Challenge-Course---AI-Track-Nanodegree-Program/blob/master/CNN_Pneumonia(Exercise).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtIVthM4gPSy",
        "colab_type": "text"
      },
      "source": [
        "# Using Pytorch CNN for Medical Image Binary Classification (Normal/Pneumonia)\n",
        "As I mentioned before, after finishing each lesson in the course I want to explore the lesson topic in depth and publish an exercise/solution notebook. I just finished Lesson 6(Convolutional Neural Networks) and decided to use the knowledge I learned to make a binary classification CNN for detecting pneumonia in chest X-Ray scans.\n",
        "You are welcome to play around with my solution and find better network architecture/better hyperparameters!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2IC5Ht_hN8M",
        "colab_type": "text"
      },
      "source": [
        "We start by downloading the dataset from Kaggle. There are a few datasets available for pneumonia detection, I decided to choose this one for simplicity - it is a learning exercise afterall.\n",
        "\n",
        "https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\n",
        "\n",
        "**Change the Kaggle username and Kaggle API key to the ones from your account**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aM9u16ahWOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"***\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"***\" # key from the json file\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        "!unzip -q chest-xray-pneumonia.zip\n",
        "!rm -rf chest_xray/__MACOSX chest_xray/chest_xray"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUy9QzU1hswL",
        "colab_type": "text"
      },
      "source": [
        "We will import the standard dependencies and also download and import barbar - a little utility script that will help us to track the training progress with Keras-like visualization.\n",
        "\n",
        "https://github.com/yusugomori/barbar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ut1I2gnnM2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install barbar\n",
        "from barbar import Bar\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_qc8WCrnM3N",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Creating transform(s) and dataloaders\n",
        "\n",
        "I would suggest you start with a single transform pipeline for training, validation and test parts of the dataset. When you are satisfied with overall perfomance, it might be a good idea to add image augumentation as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9isV9P6nM3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 2\n",
        "# how many samples per batch to load\n",
        "batch_size = 16\n",
        "\n",
        "#TODO Finish transforms\n",
        "transform = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
        "                                #add crop, center and to_tensor])\n",
        "                                \n",
        "train_data = datasets.ImageFolder('chest_xray/train', transform=transform)\n",
        "valid_data = datasets.ImageFolder('chest_xray/val', transform=transform)\n",
        "test_data = datasets.ImageFolder('chest_xray/test', transform=transform)\n",
        "\n",
        "# prepare data loaders (combine dataset and sampler)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "# specify the image classes\n",
        "classes = ['normal','pneumonia']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2ItOAbNnM3l",
        "colab_type": "text"
      },
      "source": [
        "### Visualize a Batch of Training Data\n",
        "We will create a helper function that reshapes grayscale images for us and then use matplotlib for vusializing one batch of training data. After that we can have a good look at the third image in the batch and also visualize grayscale piel values (already normalized)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYnPc-fpnM3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# helper function to un-normalize and display an image\n",
        "def imshow(img):\n",
        "    img = np.reshape(img, (448,448))\n",
        "    plt.imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYXsJJmgnM4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy() # convert images to numpy for display\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "# display 20 images\n",
        "for idx in np.arange(16):\n",
        "    ax = fig.add_subplot(2, 16/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(images[idx])\n",
        "    ax.set_title(classes[labels[idx]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq6kVCPunM4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = np.reshape(images[3], (448,448))\n",
        "from skimage.transform import resize\n",
        "img = resize(img, (32, 32))\n",
        "\n",
        "fig = plt.figure(figsize = (12,12)) \n",
        "ax = fig.add_subplot(111)\n",
        "ax.imshow(img, cmap='gray')\n",
        "width, height = img.shape\n",
        "thresh = img.max()/2.5\n",
        "for x in range(width):\n",
        "    for y in range(height):\n",
        "        val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
        "        ax.annotate(str(val), xy=(y,x),horizontalalignment='center',verticalalignment='center',color='white' if img[x][y]<thresh else 'black')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baQ6etfmnM4n",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Define the Network\n",
        "\n",
        "I spent quite a while experimenting with different network architectures before I found the one that showed satisfactory results in training. My final networks is \n",
        "Conv2d(1,16,3)\n",
        "MaxPool(2,2)\n",
        "Conv2d(16,32,3)\n",
        "MaxPool(2,2)\n",
        "Conv2d(32,64,3)\n",
        "MaxPool(2,2)\n",
        "Conv2d(64,128,3)\n",
        "MaxPool(2,2)\n",
        "Conv2d(128,256,3)\n",
        "MaxPool(2,2)\n",
        "Flatten(256*14*14)\n",
        "Dropout(0.25)\n",
        "FC(256*14*14, 256)\n",
        "Dropout(0.25)\n",
        "FC(256, 2)\n",
        "Things worth exploring in my opinion:\n",
        "-Using AveragePool instead of MaxPool\n",
        "-Using larger kernels since it's a bigger image(448x448)\n",
        "-Using Depthwise convolutions\n",
        "-Increasing number of layers and adding residual connections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5f1XZicnM4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "\n",
        "# define the CNN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        #TODO define your network\n",
        "\n",
        "    def forward(self, x):\n",
        "        #TODO define a forward pass\n",
        "        return x\n",
        "\n",
        "# create a complete CNN\n",
        "model = Net()\n",
        "print(model)\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if train_on_gpu:\n",
        "    model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqHnd6wDnM4_",
        "colab_type": "text"
      },
      "source": [
        "### Specify Loss Function and Optimizer\n",
        "I went with CrossEntropyLoss and Adam optimizer. It took me a while to find a suitable learning rate - the number turned out to be quite small in the end. I also tried BCELoss(binary classification loss) with sigmoid activation function for the last layer, but the results didn't meet my expectations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNnMPZzenM5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# TODO specify loss function\n",
        "\n",
        "\n",
        "# TODO specify optimizer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvxiPnYBnM5P",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Train the Network\n",
        "\n",
        "Remember to look at how the training and validation loss decreases over time; if the validation loss ever increases it indicates possible overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWQ94-5YnM5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 30 # you may increase this number to train a final model\n",
        "\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "train_losses, test_losses = [], []\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    print('Epoch: {}'.format(epoch))\n",
        "    # keep track of training and validation loss\n",
        "    running_loss = 0\n",
        "    valid_loss = 0\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()\n",
        "    for data, target in Bar(train_loader):\n",
        "        batch_num = batch_num + 1\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "      for images, labels in valid_loader:\n",
        "        if train_on_gpu:\n",
        "          images, labels = images.cuda(), labels.cuda()\n",
        "        ps = model(images)\n",
        "        top_p, top_class = ps.topk(1, dim=1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        test_loss += criterion(ps, labels)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "      train_losses.append(running_loss/len(train_loader))\n",
        "      test_losses.append(test_loss/len(valid_loader))\n",
        "      print(\"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
        "      \"Test Loss: {:.3f}.. \".format(test_losses[-1]),\n",
        "      \"Test Accuracy: {:.3f}\".format(accuracy/len(valid_loader)))\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if test_losses[-1] <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        test_losses[-1]))\n",
        "        torch.save(model.state_dict(), 'model_pneumonia.pt')\n",
        "        valid_loss_min = test_losses[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9boOc67nM5a",
        "colab_type": "text"
      },
      "source": [
        "###  Load the Model with the Lowest Validation Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vwPZIGtnM5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load('model_pneumonia.pt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhF6o5ftnM5j",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Test the Trained Network\n",
        "\n",
        "Test your trained model on previously unseen data! For my model it has good accuracy for pneumonia and not so good for normal lungs. Well, better be cautios then negligent I guess :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGR1RxHbnM5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# track test loss\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "model.cpu()\n",
        "model.eval()\n",
        "# iterate over test data\n",
        "for data, target in test_loader:\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(batch_size):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(2):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            classes[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45qUd-5unM5x",
        "colab_type": "text"
      },
      "source": [
        "### Visualize Sample Test Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaL666BVnM5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "model.cpu()\n",
        "# move model inputs to cuda, if GPU available\n",
        "\n",
        "# get sample outputs\n",
        "output = model(images)\n",
        "#convert output probabilities to predicted class\n",
        "_, preds_tensor = torch.max(output, 1)\n",
        "preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
        "# plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(16):\n",
        "    ax = fig.add_subplot(2, 16/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(images[idx])\n",
        "    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
        "                 color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}